---
title: "Regression_1"
author: "Dr K"
date: "April 25, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(tidyverse)
require(tigerstats)
```

## First Read in the data

```{r}
theData <- read.csv(file="myData.csv",header = TRUE)
```


Now we are gonna look at what relations exist between our variables with plots.




## Now we see why ggplot is so cool
```{r}
basicNN <- ggplot(theData,aes(x=EXAM1,y=FINAL))
```
## Now add scatterplot and fitted regression line  

```{r}
basicNN + geom_point() 
basicNN + geom_point() + geom_lm()
```
  
Lets take a look at how exam2 predicts the final

```{r}
basicN2 <- ggplot(theData,aes(x=EXAM2,y=FINAL))
```
## Now add scatterplot and fitted regression line  

```{r}
basicN2 + geom_point() 
basicN2 + geom_point() + geom_lm()
``` 
  
Lets take a look at how exam3 predicts the final

```{r}
basicN3 <- ggplot(theData,aes(x=EXAM3,y=FINAL))
```
## Now add scatterplot and fitted regression line  

```{r}
basicN3 + geom_point() 
basicN3 + geom_point() + geom_lm()
```
  
# Build 3 regression Models and determine how well each predicts the Final

```{r}
M1 <- lm(FINAL ~ EXAM1, data=theData) 
summary.lm(M1)
summary(M1)$adj.r.squared
```
```{r}
M2 <- lm(FINAL ~ EXAM2, data=theData) 
summary.lm(M2)
summary(M2)$adj.r.squared
```  
```{r}
M3 <- lm(FINAL ~ EXAM3, data=theData) 
summary.lm(M3)
M3r2 <- summary(M3)$adj.r.squared
```
## Based on comparing adjusted $R^2$ model M3 > M1 > M2  
with M3 explaining `r round(summary(M3)$adj.r.squared * 100,1)` percent of the error  
whereas M1 Explained `r round(summary(M1)$adj.r.squared * 100,1)` percent and  
M2 Explained `r round(summary(M2)$adj.r.squared * 100,1)` percent

## How to Predict using the model  

Suppose student Paul got 75 on EXAM1 92 on EXAM2 and 83 on EXAM3.
Student Mary got 92 on EXAM1 93 on EXAM2 and 79 on EXAM3

What does each of our models predict for their respective FINAL?  

### We use the predict.lm function "predict.lm(model,newdata)"  
We need to create a newdata frame that has variables named the same as those we used to make our model.  

```{r}
new <- data.frame(name=c("Paul","Mary"),EXAM1=c(75,92),EXAM2=c(92,93),EXAM3=c(83,79))
```

#### Now lets see what the three models predict - Here is M1

```{r}
predict.lm(M1,new)
```

####  Here is M2


```{r}
predict.lm(M2,new)
``` 

####  Here is M3


```{r}
predict.lm(M3,new)
```  
## Upping the game to more than one predictor  
#### It seems intuitive that knowing the score on more that one exam should improve our ability to predict the final --- how does that work?  

We start by making a model using the first two exams calling it M12  

```{r}
M12 <- lm(FINAL ~ EXAM1 + EXAM2, data = theData)
summary.lm(M12)
```
The adjusted R^2 is larger than either tha of M1 or M2, however the is a statistical test "anova" wich is commonly used to test nested models. It is called "anova" in R  

```{r}
anova(M1,M12)
anova(M2,M12)
```


The test being performed is called a "Drop in F test" It measures whether the sums of squares of our errors (ie the residuals) has been significantly reduced. We can see that all the errors from only EXAM1 amounted to around 1456 and they dropped to 1020. This shows the drop of over 400 points to be significant. The drop for going from EXAM2 to both went from 1888 down to 1020 - larger drop with more significance. 

## How about prediction?

```{r}
predict.lm(M12,new)
```
  
### Lets explore some other prediction capabilities eg confidence   

```{r}
predict.lm(M12,new,interval = "confidence",se.fit = TRUE)
```
### and now prediction  

```{r}
predict.lm(M12,new,interval = "prediction",se.fit = TRUE)
```
### By default the intervals are at the 95% level here we change it to 90%  

```{r}
predict.lm(M12,new,interval = "prediction",level = 0.90,se.fit = TRUE)
```
#### Prediction intervals are always larger  

a prediction interval will be wider than a confidence interval. The difference between a prediction interval and a confidence interval is focus. ... The key point is that the confidence interval tells you about the likely next prediction irrespective of regressor value and the prediction interval talks about the next point at that regressor value.  
  
### Now we build a fancier plot showing confidence and prediction intervals -- see if you can figure out what the code is doing.  

```{r}
temp_var <- predict(M1,interval="confidence") 
new_df <- cbind(theData, temp_var) %>% rename(lwrc=lwr,uprc=upr,fitc=fit)
temp_var2 <- predict(M1,interval="prediction") 
new_df2 <- cbind(new_df, temp_var2)  
ggplot(new_df2, aes(EXAM1, FINAL))+
  geom_point()+
geom_line(aes(y=lwrc), color = "blue", linetype = "dashed")+
    geom_line(aes(y=uprc), color = "blue", linetype = "dashed")+
 geom_line(aes(y=lwr), color = "red", linetype = "dashed")+
    geom_line(aes(y=upr), color = "red", linetype = "dashed")+ 
    geom_smooth(method=lm, se=TRUE)

```


## Comparing models works on nested models - try it just between M1 and M2  
#### M1 and M2 are not nested (they share no common predictors)  


```{r}
anova(M1,M2)
```

no F test is performed but you still get information on residual sums of squares from your models. 

## Now your turn  
### Create a model M123 that uses all the exams. Is it the best?? What is the RSS and Adj $R^2$ for it?